{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiquealmeida/public-colab-notebooks/blob/main/Replicate%20Test%2001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running BAAI/bge-large-en-v1.5 on Replicate\n",
        "\n",
        "In this notebook, we'll see how to run [`BAAI/bge-large-en-v1.5`](https://hf.co/baai/bge-large-en-v1.5) on Replicate - the current SOTA open source model for text embeddings! (as of 10/27/23)\n",
        "\n",
        "As you'll see, this Replicate model is both better than OpenAI embeddings, and 4x cheaper to run for large scale text embedding.\n",
        "\n",
        "ðŸ‘€ See the model in the Replicate UI [here](https://replicate.com/nateraw/bge-large-en-v1.5), and more ways to run it (node, curl, docker, etc.) [here](https://replicate.com/nateraw/bge-large-en-v1.5/api)."
      ],
      "metadata": {
        "id": "W43eHJBxYNyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eZv5qXwMUuR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install replicate\n",
        "\n",
        "# to count tokens\n",
        "! pip install transformers sentencepiece\n",
        "\n",
        "# For our example dataset samsum, we need these\n",
        "! pip install datasets py7zr scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate with [Replicate](https://replicate.com) :)"
      ],
      "metadata": {
        "id": "g3AmFS0bfyVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = getpass(\"Enter your Replicate API Token from:\\nhttps://replicate.com/account/api-tokens\\n\\nPress Enter when done\\n\")"
      ],
      "metadata": {
        "id": "HM_T3HB_iUX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From list of text\n",
        "\n",
        "Quick example from JSON list of text.\n",
        "\n",
        "Run this to get the model warmed up. Read about how cold boots work on Replicate [here](https://replicate.com/docs/how-does-replicate-work#cold-boots)."
      ],
      "metadata": {
        "id": "e6JfZ0VxnCzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import replicate\n",
        "\n",
        "texts = [\n",
        "    \"the happy cat\",\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"lorem ipsum dolor sit amet\",\n",
        "    \"this is a test\",\n",
        "]\n",
        "\n",
        "output = replicate.run(\n",
        "    \"nateraw/bge-large-en-v1.5:9cf9f015a9cb9c61d1a2610659cdac4a4ca222f2d3707a68517b18c198a9add1\",\n",
        "    input={\"texts\": json.dumps(texts)}\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "oQEYBJAEjSNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From jsonl file\n",
        "\n",
        "I recommend to use a file for making predictions if you've got a larger amount of text to embed (>100 embeddings).\n",
        "\n",
        "Here's a dummy example to show you the best way to do that."
      ],
      "metadata": {
        "id": "jSmrfokvm9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dummy_example.jsonl\n",
        "{\"text\": \"the happy cat\"}\n",
        "{\"text\": \"the quick brown fox jumps over the lazy dog\"}\n",
        "{\"text\": \"lorem ipsum dolor sit amet\"}\n",
        "{\"text\": \"this is a test\"}"
      ],
      "metadata": {
        "id": "xMCse4gBCcf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = replicate.run(\n",
        "    \"nateraw/bge-large-en-v1.5:9cf9f015a9cb9c61d1a2610659cdac4a4ca222f2d3707a68517b18c198a9add1\",\n",
        "    input={\"path\": open(\"dummy_example.jsonl\", \"rb\")}\n",
        ")"
      ],
      "metadata": {
        "id": "0T6xRbASSeeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(output)"
      ],
      "metadata": {
        "id": "Xb21Cmu3S0wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Example - big jsonl file (via `datasets` library)\n",
        "\n",
        "Here, we'll encode the whole [samsum](https://hf.co/datasets/samsum) dataset. ~14k examples."
      ],
      "metadata": {
        "id": "unpnmgD4S4mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"samsum\"\n",
        "text_field = \"dialogue\"\n",
        "outfile_name = \"samsum_dialogue.jsonl\"\n",
        "\n",
        "ds = load_dataset(dataset_name, split='train')\n",
        "ds = ds.remove_columns([x for x in ds.column_names if x != text_field])\n",
        "ds = ds.rename_column(text_field, \"text\")\n",
        "texts = ds[\"text\"]\n",
        "texts[0]"
      ],
      "metadata": {
        "id": "aiMx7kaVnMCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write to jsonl text file!"
      ],
      "metadata": {
        "id": "8wJZ13nHTRq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.to_json(outfile_name)"
      ],
      "metadata": {
        "id": "XRB664KXBeTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like this!"
      ],
      "metadata": {
        "id": "IU1N_Z13THQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! head -n 5 {outfile_name}"
      ],
      "metadata": {
        "id": "4zeB71T2TEdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Predictions\n",
        "\n",
        "This time, we'll choose to `convert_to_numpy`, which means our response will be a path to a saved `.npy` file instead of embeddings themselves. This is recommended when you want to compute a lot of embeddings at once, like we're doing here."
      ],
      "metadata": {
        "id": "nVNyjCMrtsf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "output = replicate.run(\n",
        "    \"nateraw/bge-large-en-v1.5:9cf9f015a9cb9c61d1a2610659cdac4a4ca222f2d3707a68517b18c198a9add1\",\n",
        "    input=dict(\n",
        "        path=open(outfile_name, \"rb\"),\n",
        "        convert_to_numpy=True,\n",
        "        batch_size=64\n",
        "    )\n",
        ")\n",
        "time_to_embed = time.time() - start\n",
        "print(f\"that took {time_to_embed:.2f} seconds.\")\n",
        "\n",
        "print(\"output\", output)"
      ],
      "metadata": {
        "id": "gr0dZTq2w4_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the predictions\n",
        "\n",
        "Since we chose to convert to numpy, we'll load with numpy here."
      ],
      "metadata": {
        "id": "t7n6UkXnUNbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "embeds = np.load(BytesIO(requests.get(output).content))\n",
        "embeds.shape"
      ],
      "metadata": {
        "id": "keTouNXZxFAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Price vs. OpenAI"
      ],
      "metadata": {
        "id": "qdHx6g68sIEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pricing vs OpenAI\n",
        "\n",
        "At the time of writing this, OpenAI's Ada v2 model costs $0.0001 / 1K tokens."
      ],
      "metadata": {
        "id": "VjnHdIQhuCoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Model\tUsage\n",
        "Ada v2\t$0.0001 / 1K tokens\n",
        "```"
      ],
      "metadata": {
        "id": "UgM4lPvpuAD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On replicate, you're charged by the second for the hardware you're running on. In this case, we're using A40 (Large) instances, which cost 0.000725/sec.\n",
        "\n",
        "ðŸ‘€ Read more about Replicate's pricing [here](https://replicate.com/pricing).\n",
        "\n",
        "Below, we'll compare both"
      ],
      "metadata": {
        "id": "7x2gXIztu0gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")"
      ],
      "metadata": {
        "id": "657ZxyS-ss2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare a benchmark file with 512 tokens in each example (the max seq length of this model).\n",
        "\n",
        "Our benchmark will have 5,120,000 tokens."
      ],
      "metadata": {
        "id": "fLMW_c3VbyUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, \\\n",
        "sed do eiusmod tempor a b\n",
        "\"\"\" * 16  # Not long enough, need >= 512 tokens, so multiply by 16\n",
        "\n",
        "# no truncation (how many input tokens)\n",
        "print(len(tokenizer.encode(text, truncation=False, add_special_tokens=False)))\n",
        "# with truncation (just for fun)\n",
        "print(len(tokenizer.encode(text, truncation=True, add_special_tokens=False)))"
      ],
      "metadata": {
        "id": "4B5qT0DJZfnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_dict({\"text\": [text] * 10000})"
      ],
      "metadata": {
        "id": "iNMUGmFWasLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(ex):\n",
        "    ex['num_tokens'] = len(tokenizer.encode(ex[\"text\"], truncation=True, add_special_tokens=False))\n",
        "    return ex\n",
        "\n",
        "ds = ds.map(count_tokens)"
      ],
      "metadata": {
        "id": "5zBi5GFYsyla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_tokens = sum(ds['num_tokens'])\n",
        "total_tokens"
      ],
      "metadata": {
        "id": "D4wZY0R6tO4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outfile_name = \"benchmark.jsonl\"\n",
        "ds.to_json(outfile_name)"
      ],
      "metadata": {
        "id": "RQ0Hoc0lbh4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ydjah_rUezDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll run the model using `replicate.predictions.create`, which will return a prediction object that we can use to get the actual time our run is billed for. This way, we can accurately calculate the cost."
      ],
      "metadata": {
        "id": "qOUIMaLeeYJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = replicate.models.get(\"nateraw/bge-large-en-v1.5\")\n",
        "version = model.latest_version\n",
        "prediction = replicate.predictions.create(\n",
        "    version,\n",
        "    input=dict(\n",
        "        path=open(outfile_name, \"rb\"),\n",
        "        convert_to_numpy=True,\n",
        "        batch_size=64\n",
        "    )\n",
        ")\n",
        "prediction.wait()"
      ],
      "metadata": {
        "id": "WkP67KK9bZvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_to_embed = prediction.metrics['predict_time']\n",
        "print(f\"that took {time_to_embed:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "gxJa1y9i2gvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_cost = 0.0001  # per 1k tokens\n",
        "openai_price = total_tokens / 1000 * openai_cost\n",
        "print(f\"OpenAI price: ${openai_price:.3f} USD\")"
      ],
      "metadata": {
        "id": "9uAT0unBuPVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicate_price = time_to_embed * 0.000725\n",
        "print(f\"Replicate cost: ${replicate_price:.3f}\")"
      ],
      "metadata": {
        "id": "4cdr7oW1vnBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}